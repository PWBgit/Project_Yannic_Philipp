{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(209222, 37)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the data\n",
    "# the data is split up in multiple .csv-files, so we generate a single dataframe from all of them\n",
    "\n",
    "# get all filenames in the data directory\n",
    "os.chdir(os.getcwd()+'/data/data')\n",
    "all_files = list(filter(os.path.isfile, os.listdir('.')))\n",
    "\n",
    "# in case there are other files in the directory: \n",
    "# define a pattern which all relevant files follow, then make a list of 'valid' files based on their name\n",
    "regex = re.compile(r\"Kickstarter\\d+.csv\")\n",
    "valid_files = list(filter(regex.match, all_files))\n",
    "\n",
    "# read the files and concatenate a single dataframe\n",
    "df_from_each_file = (pd.read_csv(f) for f in valid_files)\n",
    "data = pd.concat(df_from_each_file, ignore_index=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we look at the target variable **state**. There are 5 categories, successful, failed, live, suspended and canceled. Since we want to predict the likelihood of success, we will only consider campaigns that had a regular finish after reaching the deadline (not suspended or canceled). Campaigns that are still live are also of no use to us. So we will drop the corresponding observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192664, 37)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop all rows where state == live/suspended/canceled, these projects are neither successful nor did they fail\n",
    "data.drop(data[data.state.isin(['live', 'suspended', 'canceled'])].index, inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly enough, some campaigns seem to be excact duplicates of one another. These rows we will also drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168979, 37)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop_duplicates(inplace=True, subset='id')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 37 columns in this data set. To make the EDA more comfortable we will now drop some columns, either because they have no meaning/significance for our business case, or they are redundant with other columns. We also need to ensure that the dataframe contains only information that is available at the start of a project (except our target variable **state**). Otherwise, we would be trying to predict whether a project succeeds or fails based on 'future information', like for example the amount of money that was pledged.\n",
    "\n",
    "We eliminate the following columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 168979 entries, 1 to 209221\n",
      "Data columns (total 15 columns):\n",
      " #   Column                  Non-Null Count   Dtype  \n",
      "---  ------                  --------------   -----  \n",
      " 0   backers_count           168979 non-null  int64  \n",
      " 1   blurb                   168977 non-null  object \n",
      " 2   category                168979 non-null  object \n",
      " 3   country                 168979 non-null  object \n",
      " 4   creator                 168979 non-null  object \n",
      " 5   currency_trailing_code  168979 non-null  bool   \n",
      " 6   goal                    168979 non-null  float64\n",
      " 7   launched_at             168979 non-null  int64  \n",
      " 8   name                    168979 non-null  object \n",
      " 9   pledged                 168979 non-null  float64\n",
      " 10  spotlight               168979 non-null  bool   \n",
      " 11  staff_pick              168979 non-null  bool   \n",
      " 12  state                   168979 non-null  object \n",
      " 13  usd_pledged             168979 non-null  float64\n",
      " 14  usd_type                168959 non-null  object \n",
      "dtypes: bool(3), float64(3), int64(2), object(7)\n",
      "memory usage: 17.2+ MB\n"
     ]
    }
   ],
   "source": [
    "data.drop(axis=1, inplace=True, labels=[\n",
    "    'currency_symbol',            # redundant with currency\n",
    "    'friends',                    # only NaN values\n",
    "    'fx_rate',                    # \n",
    "    'is_backing',                 # only NaN values\n",
    "    'slug',                       # redundant with name\n",
    "    'permissions',                # only NaN values\n",
    "    'photo',                      # we will not use image analysis, so no significance for model\n",
    "    'profile',                    # contains many values that are also in other columns, too many different profile ids to dummy\n",
    "    'static_usd_rate',            # data imbalance, usefullness to model unlikely\n",
    "    'source_url',                 # contains url for categories, thus redundant\n",
    "    'state_changed_at', \n",
    "    'deadline', \n",
    "    'id',                         # unique campaign identifier\n",
    "    'converted_pledged_amount',   # redundant with pledged\n",
    "    'created_at', \n",
    "    'is_starred', \n",
    "    'currency',                   # largely redundant with (and less detailed than) country\n",
    "    'current_currency',\n",
    "    'disable_communication',      # only 'False' values\n",
    "    'is_starrable',\n",
    "    'location',                   # creating a dummy var. for each city not feasable (we also have country for a general location)\n",
    "    'urls'                        # not used in model           \n",
    "])\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all date variables to datetime format\n",
    "data.launched_at = pd.to_datetime(data.launched_at, unit='s')\n",
    "\n",
    "# convert the objects in category into their respective category names \n",
    "data.category = data.category.apply(lambda x: ast.literal_eval(x).get('name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168979, 196)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dummies for the remaining categorial variables and merge them with the dataframe\n",
    "dummies = pd.get_dummies(data[['state', 'country', 'usd_type', 'category']], drop_first=True)\n",
    "data = pd.concat([data, dummies], axis=1)\n",
    "data.shape\n",
    "\n",
    "# the original categorial variables will be dropped later but are still relevant for the EDA\n",
    "# data.drop(labels = ['country', 'state', 'usd_type', 'category'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots(figsize=(30,30))\n",
    "\n",
    "#sns.heatmap(data.corr().round(2), annot= True,\n",
    "#            cmap = sns.diverging_palette(230, 20, as_cmap=True), \n",
    "#            mask = np.triu(np.ones_like(data.corr(), dtype=bool)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nachrichten an Philipp:\n",
    "# in der anderen EDA waren delta_created_launched und campaign_length unter den finalen Prädikatoren (vielleicht doch nicht droppen?)\n",
    "# fx_rate war aus irgendeinem Grund auch drin, dafür country nicht.\n",
    "# brauchen wir name und pledged?\n",
    "# einige Kampagnen sind doppelt im Datensatz - sind überraschen viele, habe ich erstmal entfernt!\n",
    "# data.id.value_counts(dropna=False)\n",
    "# data[data['id']==1278991287]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nf] *",
   "language": "python",
   "name": "conda-env-nf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
